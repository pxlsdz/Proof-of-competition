{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# 导入数据io操作\n",
    "from collections import deque\n",
    "from six import next\n",
    "\n",
    "# 调用reader.py\n",
    "import readers\n",
    "\n",
    "# Main imports for training\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# 评估每个轮次的训练时间\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义装饰器，监控运行时间\n",
    "def timmer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        res = func(*args, **kwargs)\n",
    "        stop_time = time.time()\n",
    "        print('函数 %s, 运行时间: %s' % (func.__name__, stop_time - start_time))\n",
    "        return res\n",
    "    return wrapper\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#二维字典的添加\n",
    "def addtwodimdict(thedict, key_a, key_b, val):\n",
    "    if key_a in thedict:\n",
    "        thedict[key_a].update({key_b: val})\n",
    "    else:\n",
    "        thedict.update({key_a:{key_b: val}})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg=0\n",
    "num=0\n",
    "@timmer\n",
    "def loadData(fp=\"data/ml-1m/ratings.dat\"):\n",
    "    data = []\n",
    "    user_set=set()\n",
    "    item_set=set()\n",
    "    \n",
    "    for l in open(fp):\n",
    "        temp=list(map(int,l.strip().split('::')))\n",
    "        \n",
    "        data.append(temp)\n",
    "        user_set.add(temp[0])\n",
    "        item_set.add(temp[1])\n",
    "    data=list(sorted(data,key=lambda x:x[3]))\n",
    "\n",
    "    return data,user_set,item_set\n",
    "\n",
    "@timmer\n",
    "def dealData(data):\n",
    "    \n",
    "    train = []\n",
    "    le=len(data)\n",
    "    rating_dict={}\n",
    "    for user, item, rating, timestamp in data:\n",
    "        addtwodimdict(rating_dict, user, item, rating)\n",
    "        global avg,num\n",
    "        avg+=rating\n",
    "        num+=1\n",
    "        train.append((user, item))\n",
    "\n",
    "    # 处理成字典的形式，user->set(items)\n",
    "    def convert_dict(data):\n",
    "        data_dict = {}\n",
    "        for user, item in data:\n",
    "            if user not in data_dict:\n",
    "                data_dict[user] = set()\n",
    "            data_dict[user].add(item)\n",
    "        data_dict = {k: list(data_dict[k]) for k in data_dict}\n",
    "        return data_dict\n",
    "\n",
    "    return convert_dict(train), rating_dict, \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数 loadData, 运行时间: 3.43342661857605\n"
     ]
    }
   ],
   "source": [
    "data,user_set,item_set=loadData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6040 3706\n",
      "函数 dealData, 运行时间: 1.6007156372070312\n"
     ]
    }
   ],
   "source": [
    "print(\"%d %d\"%(len(user_set),len(item_set)))\n",
    "u_i,rating_dict=dealData(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于复制结果的恒定种子\n",
    "np.random.seed(42)\n",
    "\n",
    "#3952  个电影 6,040个用户\n",
    "u_num = 6040 \n",
    "i_num = 3952  \n",
    "\n",
    "#一次训练所选取的样本\n",
    "batch_size = 1000 \n",
    "\n",
    "# 数据的维度\n",
    "dims = 10    \n",
    "\n",
    "# 最大迭代轮次\n",
    "max_epochs = 50   \n",
    "\n",
    "# 使用设备\n",
    "place_device = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data():\n",
    "    # 数据依次是用户ID、项目ID、评级、时间戳\n",
    "    # 样例数据：data - 3::1196::4::978297539\n",
    "    df = readers.read_file(\"data/ml-1m/ratings.dat\", sep=\"::\")\n",
    "    \n",
    "    # 获取数据的行数，待会儿要做训练和测试集的切分\n",
    "    rows = len(df)\n",
    "    \n",
    "    # 纯粹基于整数位置的索引，根据位置进行选择\n",
    "    # 实际上就是打乱一下数据的顺序 洗牌\n",
    "    df = df.iloc[np.random.permutation(rows)].reset_index(drop=True)\n",
    "    \n",
    "    # 90%用作训练，10%用作测试\n",
    "    split_index = int(rows * 0.9)\n",
    "    \n",
    "    # Use indices to separate the data\n",
    "    df_train = df[0:split_index]\n",
    "    df_test = df[split_index:].reset_index(drop=True)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(user_batch, item_batch, user_num, item_num, dim=100, device=\"/cpu:0\"):\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        # 变量域\n",
    "        with tf.variable_scope('lsi',reuse=tf.AUTO_REUSE):\n",
    "            # 全局偏置变量\n",
    "            # get_variable:在名称前面加上当前变量作用域并执行重用检查\n",
    "            bias_global = tf.get_variable(\"bias_global\",shape=[])\n",
    "            \n",
    "            # 用户的偏好\n",
    "            w_bias_user = tf.get_variable(\"embd_bias_user\", shape=[user_num])\n",
    "            # 电影的偏好\n",
    "            w_bias_item = tf.get_variable(\"embd_bias_item\", shape=[item_num])\n",
    "            \n",
    "            # 用户和电影一个batch的偏好\n",
    "            bias_user = tf.nn.embedding_lookup(w_bias_user, user_batch, name=\"bias_user\")\n",
    "            bias_item = tf.nn.embedding_lookup(w_bias_item, item_batch, name=\"bias_item\")\n",
    "            \n",
    "            # 用户和电影的权重\n",
    "            w_user = tf.get_variable(\"embd_user\", shape=[user_num, dim],\n",
    "                                     initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "            w_item = tf.get_variable(\"embd_item\", shape=[item_num, dim],\n",
    "                                     initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "            \n",
    "            # 给定批处理的用户和项的权重嵌入\n",
    "            # 用户和电影一个batch的权重\n",
    "            embd_user = tf.nn.embedding_lookup(w_user, user_batch, name=\"embedding_user\")\n",
    "            embd_item = tf.nn.embedding_lookup(w_item, item_batch, name=\"embedding_item\")\n",
    "    \n",
    "    with tf.device(device):\n",
    "        # 计算张量各维度元素和\n",
    "        infer = tf.reduce_sum(tf.multiply(embd_user, embd_item), 1)\n",
    "        \n",
    "        infer = tf.add(infer, bias_global)\n",
    "        infer = tf.add(infer, bias_user)\n",
    "        infer = tf.add(infer, bias_item, name=\"svd_inference\")\n",
    "        \n",
    "        # 加上L2的正则化项\n",
    "        # l2_loss: 计算一个张量的L2范数的一半\n",
    "        # regularizer：正则化项\n",
    "        regularizer = tf.add(tf.nn.l2_loss(embd_user), tf.nn.l2_loss(embd_item), \n",
    "                             name=\"svd_regularizer\")\n",
    "\n",
    "    # 返回我们预测的结果和正则化项\n",
    "    return infer, regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(infer, regularizer, rate_batch, learning_rate=0.002, reg=0.01, device=\"/cpu:0\"):\n",
    "    with tf.device(device):\n",
    "        # 使用L2 loss算出预测值到实际值的距离 \n",
    "        # infer  预测值    rate_batch 实际值\n",
    "        cost_l2 = tf.nn.l2_loss(tf.subtract(infer, rate_batch))\n",
    "        \n",
    "        # 惩罚的方式----L2\n",
    "        penalty = tf.constant(reg, dtype=tf.float32, shape=[], name=\"l2\")\n",
    "        \n",
    "        # 损失函数 = 数据损失（data loss） + 正则化损失（正则化项 * L2惩罚方式）\n",
    "        cost = tf.add(cost_l2, tf.multiply(regularizer, penalty))\n",
    "        \n",
    "        # 训练 使用梯度下降\n",
    "        train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    return cost, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples 900188, test samples 100021, samples per batch 900\n"
     ]
    }
   ],
   "source": [
    "# 从评级文件读取数据以构建 tensorflow 模型\n",
    "df_train, df_test = get_data()\n",
    "\n",
    "samples_per_batch = len(df_train) // batch_size\n",
    "print(\"Number of train samples %d, test samples %d, samples per batch %d\" % \n",
    "      (len(df_train), len(df_test), samples_per_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3706\n",
      "0    1696\n",
      "1    5448\n",
      "2    2242\n",
      "3    5629\n",
      "4     423\n",
      "Name: user, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# 查看前5个用户值\n",
    "se=set()\n",
    "for it in df_train[\"item\"]:\n",
    "    se.add(it)\n",
    "for it in df_test[\"item\"]:\n",
    "    se.add(it)\n",
    "print(len(se))\n",
    "print(df_test[\"user\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用shuffle迭代器生成随机批次，用于训练\n",
    "iter_train = readers.ShuffleIterator([df_train[\"user\"],\n",
    "                                     df_train[\"item\"],\n",
    "                                     df_train[\"rate\"]],\n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "# 按顺序生成一个epoch的batch用于测试\n",
    "iter_test = readers.OneEpochIterator([df_test[\"user\"],\n",
    "                                     df_test[\"item\"],\n",
    "                                     df_test[\"rate\"]],\n",
    "                                     batch_size=-1)\n",
    "\n",
    "# 创建占位符\n",
    "user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "item_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_item\")\n",
    "rate_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "infer, regularizer = model(user_batch, item_batch, user_num=u_num, item_num=i_num, dim=dims, device=place_device)\n",
    "_, train_op = loss(infer, regularizer, rate_batch, learning_rate=0.0010, reg=0.05, device=place_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "UI=[[0 for j in range(i_num)] for i in range(u_num)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tTrain Error\tVal Error\tElapsed Time\n",
      "00\t2.823\t\t1.116\t\t0.230 secs\n",
      "01\t1.048\t\t1.003\t\t0.803 secs\n",
      "02\t0.982\t\t0.967\t\t0.764 secs\n",
      "03\t0.955\t\t0.950\t\t0.760 secs\n",
      "04\t0.941\t\t0.939\t\t0.808 secs\n",
      "05\t0.932\t\t0.932\t\t0.820 secs\n",
      "06\t0.926\t\t0.928\t\t0.765 secs\n",
      "07\t0.922\t\t0.924\t\t0.826 secs\n",
      "08\t0.919\t\t0.922\t\t0.821 secs\n",
      "09\t0.915\t\t0.920\t\t0.914 secs\n",
      "10\t0.913\t\t0.918\t\t0.725 secs\n",
      "11\t0.911\t\t0.917\t\t0.723 secs\n",
      "12\t0.909\t\t0.919\t\t0.718 secs\n",
      "13\t0.908\t\t0.915\t\t0.867 secs\n",
      "14\t0.909\t\t0.913\t\t0.841 secs\n",
      "15\t0.907\t\t0.915\t\t0.783 secs\n",
      "16\t0.907\t\t0.912\t\t0.779 secs\n",
      "17\t0.905\t\t0.911\t\t0.797 secs\n",
      "18\t0.905\t\t0.911\t\t0.888 secs\n",
      "19\t0.905\t\t0.911\t\t0.871 secs\n",
      "20\t0.903\t\t0.912\t\t0.881 secs\n",
      "21\t0.903\t\t0.910\t\t0.864 secs\n",
      "22\t0.902\t\t0.910\t\t0.738 secs\n",
      "23\t0.902\t\t0.910\t\t0.841 secs\n",
      "24\t0.902\t\t0.909\t\t0.802 secs\n",
      "25\t0.903\t\t0.909\t\t0.915 secs\n",
      "26\t0.901\t\t0.909\t\t0.775 secs\n",
      "27\t0.903\t\t0.909\t\t0.769 secs\n",
      "28\t0.901\t\t0.909\t\t0.728 secs\n",
      "29\t0.901\t\t0.908\t\t0.833 secs\n",
      "30\t0.901\t\t0.909\t\t0.807 secs\n",
      "31\t0.901\t\t0.908\t\t0.778 secs\n",
      "32\t0.900\t\t0.908\t\t0.803 secs\n",
      "33\t0.900\t\t0.908\t\t0.797 secs\n",
      "34\t0.900\t\t0.909\t\t0.822 secs\n",
      "35\t0.900\t\t0.908\t\t0.778 secs\n",
      "36\t0.901\t\t0.908\t\t0.762 secs\n",
      "37\t0.901\t\t0.910\t\t0.789 secs\n",
      "38\t0.900\t\t0.908\t\t0.776 secs\n",
      "39\t0.900\t\t0.908\t\t0.727 secs\n",
      "40\t0.899\t\t0.909\t\t0.755 secs\n",
      "41\t0.899\t\t0.908\t\t0.830 secs\n",
      "42\t0.900\t\t0.908\t\t0.832 secs\n",
      "43\t0.900\t\t0.908\t\t0.829 secs\n",
      "44\t0.899\t\t0.909\t\t0.853 secs\n",
      "45\t0.899\t\t0.908\t\t0.790 secs\n",
      "46\t0.899\t\t0.908\t\t0.730 secs\n",
      "47\t0.898\t\t0.909\t\t0.718 secs\n",
      "48\t0.900\t\t0.907\t\t0.829 secs\n",
      "49\t0.899\t\t0.911\t\t0.793 secs\n",
      "()\n",
      "(6040,)\n",
      "embd_bias_user.csv\n",
      "(3952,)\n",
      "embd_bias_item.csv\n",
      "(6040, 10)\n",
      "embd_user.csv\n",
      "(3952, 10)\n",
      "embd_item.csv\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(\"%s\\t%s\\t%s\\t%s\" % (\"Epoch\", \"Train Error\", \"Val Error\", \"Elapsed Time\"))\n",
    "    errors = deque(maxlen=samples_per_batch)\n",
    "    start = time.time()\n",
    "   \n",
    "    for i in range(max_epochs * samples_per_batch):\n",
    "        users, items, rates = next(iter_train)\n",
    "  \n",
    "        _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch: users,\n",
    "                                                               item_batch: items,\n",
    "                                                               rate_batch: rates})\n",
    "        pred_batch = clip(pred_batch)\n",
    "        errors.append(np.power(pred_batch - rates, 2))\n",
    "        \n",
    "        if i % samples_per_batch == 0:\n",
    "            train_err = np.sqrt(np.mean(errors))\n",
    "            test_err2 = np.array([])\n",
    "            for users, items, rates in iter_test:\n",
    "                pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                        item_batch: items})\n",
    "                pred_batch = clip(pred_batch)\n",
    "                #print(pred_batch)\n",
    "                test_err2 = np.append(test_err2, np.power(pred_batch - rates, 2))\n",
    "\n",
    "                \n",
    "            \n",
    "            end = time.time()\n",
    "            \n",
    "            print(\"%02d\\t%.3f\\t\\t%.3f\\t\\t%.3f secs\" % (i // samples_per_batch, train_err, np.sqrt(np.mean(test_err2)), end - start))\n",
    "            start = end\n",
    "            \n",
    "    for var in tf.global_variables():\n",
    "        print(var.shape)\n",
    "        f_name=var.name.split('/')[-1].split(':')[0]  \n",
    "        data_numpy=var.eval()\n",
    "        \n",
    "        if f_name==\"bias_global\":continue\n",
    "        f_name=f_name+\".csv\"\n",
    "        print(f_name)\n",
    "        np.savetxt(f_name, data_numpy, delimiter=\",\")\n",
    "\n",
    "                \n",
    "    saver.save(sess, './save/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(fileName,n,m):\n",
    "    f = open(fileName)\n",
    "    rownum=0\n",
    "    train_X_matrix = np.empty((n,m),np.float64)\n",
    "    for line in f.readlines():\n",
    "        train_X_matrix[rownum] = np.asarray(line.strip('\\n ').split(','), dtype=float)\n",
    "        rownum += 1\n",
    "    return train_X_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 6040)\n",
      "(3952, 10)\n"
     ]
    }
   ],
   "source": [
    "P=loadDataSet(\"embd_user.csv\",u_num,10).T\n",
    "Q=loadDataSet(\"embd_item.csv\",i_num,10)\n",
    "u_ba=loadDataSet(\"embd_bias_item.csv\",u_num,1)\n",
    "i_ba=loadDataSet(\"embd_bias_item.csv\",i_num,1)\n",
    "print(P.shape)\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "UI=np.dot(Q,P)\n",
    "for i in range(0,i_num):\n",
    "    for j in range(0,u_num):\n",
    "        \n",
    "        if i+1 in rating_dict:\n",
    "            if j+1 in rating_dict[i+1]:\n",
    "                UI[i][j]=rating_dict[i+1][j+1]\n",
    "                continue\n",
    "                \n",
    "        UI[i][j]+=u_ba[j]+i_ba[i]+avg/num\n",
    "    \n",
    "np.savetxt(\"UI.csv\", UI, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Bio.Cluster import * \n",
    "#用k-means 或 k-median进行聚类，K为聚类数\n",
    "def KCluster(data, K=10):\n",
    "    clusterid, error, nfound = kcluster (data,npass=100,nclusters=K,mask=None, weight=None,  dist='c', initialid=None)\n",
    "    cdata, cmask = clustercentroids(data, mask=None, clusterid=clusterid)\n",
    "    #silhouette_avg = silhouette_score(data, clusterid)\n",
    "    return clusterid,cdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "UI=np.clip(UI, 0.0, 5.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clusterid,cdata=KCluster(UI, 10)\n",
    "best_clusterid=clusterid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_d=[0 for i in range(0,i_num+1)]\n",
    "out_d=[0 for i in range(0,i_num+1)]\n",
    "\n",
    "for  u in u_i:\n",
    "    pre=-1\n",
    "    for it in u_i[u]:\n",
    "        it-=1\n",
    "        if pre==-1:\n",
    "            in_d[it]+=1\n",
    "            pre=it\n",
    "            continue\n",
    "        \n",
    "        if best_clusterid[it]!=best_clusterid[pre]:\n",
    "\n",
    "            out_d[pre]+=1\n",
    "            in_d[it]+=1\n",
    "        pre=it\n",
    "        \n",
    "    \n",
    "        \n",
    "   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.DataFrame(data=best_clusterid)\n",
    "test.to_csv('best_clusterid.csv')\n",
    "test=pd.DataFrame(data=cdata)\n",
    "test.to_csv('cdata.csv')\n",
    "test=pd.DataFrame(data=in_d)\n",
    "test.to_csv('in_d.csv')\n",
    "test=pd.DataFrame(data=out_d)\n",
    "test.to_csv('out_d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.35100338 3.10462087 2.70600504 ... 2.71827485 2.7180628  2.71790085]\n",
      " [4.72112812 4.31051225 4.13763183 ... 4.49921034 4.49886763 4.49974214]\n",
      " [4.32173394 3.41650357 3.24252654 ... 3.58340725 3.5834713  3.58335983]\n",
      " ...\n",
      " [4.0375928  2.98763645 2.81762606 ... 3.03420124 3.0343395  3.03406476]\n",
      " [3.92815612 3.33833789 3.30075689 ... 4.20416056 4.20403103 4.20442974]\n",
      " [4.28114269 3.49120515 3.37108617 ... 3.86010284 3.86012074 3.86016982]]\n"
     ]
    }
   ],
   "source": [
    "print(cdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
